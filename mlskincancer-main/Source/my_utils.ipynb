{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "get_ipython().run_line_magic('matplotlib', 'inline')\n",
    "import matplotlib.pyplot as plt # Used to plot images\n",
    "\n",
    "from os import listdir # Used to access images folders\n",
    "from os.path import isfile, join # Used to access images files\n",
    "from tensorflow.keras import backend as K # Keras backed to de-allocate memory\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau # Reduce learning rate epoch by epoch\n",
    "from keras.preprocessing.image import ImageDataGenerator # Used to populate images\n",
    "from tensorflow.keras.callbacks import EarlyStopping #for deciding the number of epochs\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint #for saving the weights\n",
    "\n",
    "# Stochastic Gradient Descent\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "#Support Vector Machine\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#it returns an array with the cancer labels and a dataframe with the total number images per label for the test set\n",
    "def y_actual_labels(folder_path = 'augmented_dataset/test/test_images'):\n",
    "    \"\"\" Returns the actual labels of the test set \"\"\"\n",
    "    y_actual = []\n",
    "    test_counters = pd.DataFrame([[0,0]], columns=['benign', 'malignant'], index=['counter'])\n",
    "\n",
    "    for f in listdir(folder_path):\n",
    "        image_path = join(folder_path, f)\n",
    "    \n",
    "        if isfile(image_path):\n",
    "            if f.startswith('benign'):\n",
    "                y_actual.append('benign')\n",
    "                test_counters['benign'] += 1\n",
    "            elif f.startswith('malignant'):\n",
    "                y_actual.append('malignant')\n",
    "                test_counters['malignant'] += 1\n",
    "                \n",
    "    return y_actual, test_counters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix function definition\n",
    "def confusion_matrix(y_actual, y_pred):\n",
    "    \"\"\" Creates a confusion matrix \"\"\"\n",
    "    conf_mat = pd.DataFrame([[0, 0], [0, 0]],                            columns=['true_benign', 'true_malignant'],                            index=['predicted_benign', 'predicted_malignant'])\n",
    "    \n",
    "\n",
    "    for i in range(len(y_pred)):\n",
    "        conf_mat['true_' + y_actual[i]]['predicted_' + y_pred[i]] += 1\n",
    "        \n",
    "    return conf_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#it returns a dataframe containing precision , recall , f1-score and support metrics\n",
    "def classification_report(y_actual, y_pred):\n",
    "    from sklearn.metrics import classification_report\n",
    "    cr = classification_report(y_actual, y_pred, output_dict=True)\n",
    "\n",
    "    cr['accuracy'] = {'f1-score': cr['accuracy']}\n",
    "    for k, v in cr['macro avg'].items():\n",
    "        if k == 'f1-score':\n",
    "            continue\n",
    "        elif k == 'support':\n",
    "            cr['accuracy'][k] = v\n",
    "        else:\n",
    "            cr['accuracy'][k] = 'NA'\n",
    "            \n",
    "    return pd.DataFrame(cr).transpose()[['precision', 'recall', 'f1-score', 'support']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#returning a plot with validation and train values per epoch\n",
    "def plot_model(train_history, validation_history, what = '', where = 'upper left'):\n",
    "    \"\"\" Plots model's metrics from its trainning and validation \"\"\"\n",
    "    epochs = len(train_history)\n",
    "    plt.plot([x+1 for x in range(epochs)], train_history)\n",
    "    plt.plot([x+1 for x in range(epochs)], validation_history)\n",
    "   \n",
    "    plt.title('model ' + what)\n",
    "    plt.ylabel(what)\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'validation','test'], loc = where)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#method for neural networks, train, validation and test\n",
    "def run_nn(model, epochs = 10, batch_size = 64, rescale_factor = 1./255, lr = 1e-5, name = ''):\n",
    "    \"\"\" Trains and evaluates a model \"\"\"\n",
    "    np.random.seed(123)\n",
    "    result = {}\n",
    "    # Set a learning rate annealer\n",
    "    # This will reduce the learning rate of the ann to the half\n",
    "    # of the learning rate of the previous epoch up to the lr/100 ((1e-5)/100 = 1e-7)\n",
    "    learning_rate_reduction = ReduceLROnPlateau(monitor = 'accuracy', patience = 3,\n",
    "                                                verbose = 1, factor = 0.5,\n",
    "                                                min_lr = lr / 100)\n",
    "    \n",
    "    # Referencing from image Keras ImageDatagenerator source code, the parameter rescale is to multiply every \n",
    "    #pixel in the preprocessing image. ... Since 255 is the maximin pixel value. Rescale 1./255 is to transform every\n",
    "    #pixel value from range [0,255] -> [0,1].\n",
    "    train_datagen = ImageDataGenerator(rescale = rescale_factor)\n",
    "    valid_datagen = ImageDataGenerator(rescale = rescale_factor)\n",
    "\n",
    "    train_generator = train_datagen.flow_from_directory('augmented_dataset/train',\n",
    "                                                        batch_size = batch_size,\n",
    "                                                        class_mode = 'categorical')\n",
    "    validation_generator = valid_datagen.flow_from_directory('augmented_dataset/valid',\n",
    "                                                             batch_size = batch_size,\n",
    "                                                             class_mode = 'categorical')\n",
    "\n",
    "    # Number of batches in train set:\n",
    "    STEP_SIZE_TRAIN = train_generator.n // train_generator.batch_size\n",
    "    # Number of batches in validation set:\n",
    "    STEP_SIZE_VALID = validation_generator.n // validation_generator.batch_size\n",
    "\n",
    "    # Set a learning rate annealer\n",
    "    # This will reduce the learning rate of the ann (depending on its accurcay) to the half\n",
    "    # of the 'factor' * (learning rate of the previous epoch) up to the 'min_lr'\n",
    "    # every 'patience' epochs.\n",
    "    # 'Verbose = 1' means that we want to print a message whenever a lr reduction happens\n",
    "    learning_rate_reduction = ReduceLROnPlateau(monitor = 'accuracy', patience = 5,\n",
    "                                                verbose = 1, factor = 0.5,\n",
    "                                                min_lr = lr / 100)\n",
    "    #early stopping for avoiding overfitting and deciding number of epochs\n",
    "    early_stopping = EarlyStopping(monitor='val_loss',\n",
    "                              min_delta=0,\n",
    "                              patience=3,\n",
    "                              verbose=0, mode='auto')\n",
    "    #checkpoint for saving the weights of a model\n",
    "    checkpoint_filepath = 'C:/Users/astar/Desktop/dokimi/mlskincancer-main/Source/CheckPoint'\n",
    "    model_checkpoint_callback = ModelCheckpoint(\n",
    "                            filepath=checkpoint_filepath,\n",
    "                            save_weights_only=True,\n",
    "                            monitor='val_loss',\n",
    "                            mode='max',\n",
    "                            save_best_only=True)\n",
    "\n",
    "\n",
    "    \n",
    "    # Fit the model\n",
    "    print('Start training process for the ' + name + ' model...')\n",
    "    if name== 'CNN':\n",
    "        history = model.fit(train_generator, steps_per_epoch = STEP_SIZE_TRAIN, epochs = epochs,\n",
    "                            validation_data = validation_generator, validation_steps = STEP_SIZE_VALID,\n",
    "                            callbacks = [learning_rate_reduction,early_stopping,model_checkpoint_callback])\n",
    "    else:\n",
    "         history = model.fit(train_generator, steps_per_epoch = STEP_SIZE_TRAIN, epochs = epochs,\n",
    "                            validation_data = validation_generator, validation_steps = STEP_SIZE_VALID,\n",
    "                            callbacks = [learning_rate_reduction,early_stopping])\n",
    "    result['history'] = history\n",
    "    print(name + ' model training process was successful!')\n",
    "    \n",
    "    # Evaluate the model over validation set\n",
    "    print('Start evaluating the ' + name + ' model over validation set...')\n",
    "    metrics = model.evaluate(validation_generator, steps = STEP_SIZE_VALID)\n",
    "    result['metrics'] = {k: v for k, v in zip(model.metrics_names, metrics)}\n",
    "    print(name + ' model was evaluated on validation set successfully!')\n",
    "    \n",
    "    # Evaluate the model over test set\n",
    "    print('Start evaluating the ' + name + ' model on test set...')\n",
    "    test_datagen = ImageDataGenerator(rescale = rescale_factor)\n",
    "\n",
    "    test_generator = test_datagen.flow_from_directory(directory = 'augmented_dataset/test',\n",
    "                                                      batch_size = 1, color_mode=\"rgb\",\n",
    "                                                      class_mode = None,\n",
    "                                                      shuffle = False,\n",
    "                                                      seed = 42)\n",
    "\n",
    "    STEP_SIZE_TEST = test_generator.n // test_generator.batch_size\n",
    "\n",
    "    test_generator.reset()\n",
    "    y_pred = model.predict(test_generator, steps = STEP_SIZE_TEST, verbose = 1)\n",
    "    \n",
    "    predicted_class_indices = np.argmax(y_pred, axis = 1)\n",
    "    labels = (train_generator.class_indices)\n",
    "    labels = dict((v,k) for k, v in labels.items())\n",
    "    predictions = [labels[k] for k in predicted_class_indices]\n",
    "    result['predictions'] = predictions\n",
    "    print(name + ' model was evaluated on test set successfully!')\n",
    "    \n",
    "    result['model'] = model # The model is now trained\n",
    "    \n",
    "    if name== 'CNN':\n",
    "        # serialize weights to HDF5\n",
    "        model.save_weights(\"C:/Users/astar/Desktop/dokimi/mlskincancer-main/Source/model_weights.h5\")\n",
    "        print(\"Saved model to disk\")\n",
    " \n",
    "    \n",
    "    return result\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#method for non neural networks, train, validation and test\n",
    "\n",
    "def run_not_nn(epochs = 10, input_dim=(256, 256, 3), batch_size = 64, alpha = 1e-5, rescale_factor = 1./255, name = ''):\n",
    "    import time\n",
    "    np.random.seed(123)\n",
    "    \n",
    "    train_datagen = ImageDataGenerator(rescale = 1./255)\n",
    "    valid_datagen = ImageDataGenerator(rescale = 1./255)\n",
    "\n",
    "    train_generator = train_datagen.flow_from_directory('augmented_dataset/train',\n",
    "                                                        batch_size = batch_size,\n",
    "                                                        class_mode = 'categorical')\n",
    "    validation_generator = train_datagen.flow_from_directory('augmented_dataset/valid',\n",
    "                                                             batch_size = batch_size,\n",
    "                                                             class_mode = 'categorical')\n",
    "    if name=='SGD' :\n",
    "        clf = SGDClassifier(alpha = alpha)\n",
    "    else:\n",
    "        clf = SVC(random_state = 123, kernel = 'rbf')\n",
    "        \n",
    "   \n",
    "    STEP_SIZE_TRAIN = train_generator.n // batch_size\n",
    "    STEP_SIZE_VALID = validation_generator.n // validation_generator.batch_size\n",
    "    flat_dim = np.prod(input_dim)\n",
    "    history = []\n",
    "\n",
    "    print('Start training process for the ' + name + ' model...')\n",
    "     \n",
    "    for e in range(1, epochs + 1):\n",
    "        print('Epoch ' + str(e) + '/' + str(epochs) + ' is evaluating...', end='\\r')\n",
    "    \n",
    "        batch_num = 0\n",
    "        start = time.time()\n",
    "        for x_batch, y_batch in train_generator:\n",
    "            batch_num += 1\n",
    "    \n",
    "            batch_x = [x.reshape(flat_dim,) for x in x_batch]\n",
    "            batch_y = [np.argmax(y) for y in y_batch]\n",
    "        \n",
    "            clf.fit(batch_x, batch_y)\n",
    "    \n",
    "            if batch_num > STEP_SIZE_TRAIN:\n",
    "                break\n",
    "        \n",
    "        valid_predictions = 0\n",
    "        predictions_made = 0\n",
    "        validation_batch_num = 0\n",
    "        \n",
    "        for x_batch_validation, y_batch_validation in validation_generator:\n",
    "            validation_batch_num += 1\n",
    "            \n",
    "            batch_x_validation = [x.reshape(np.prod(input_dim),) for x in x_batch_validation]\n",
    "            batch_y_validation = [np.argmax(y) for y in y_batch_validation]\n",
    "            \n",
    "            epoch_predictions = clf.predict(batch_x_validation)\n",
    "            \n",
    "            for i in range(len(epoch_predictions)):\n",
    "                predictions_made += 1\n",
    "                if epoch_predictions[i] == batch_y_validation[i]:\n",
    "                    valid_predictions += 1\n",
    "                    \n",
    "            if validation_batch_num > STEP_SIZE_VALID:\n",
    "                break\n",
    "                    \n",
    "        history.append(valid_predictions / float(predictions_made))\n",
    "                \n",
    "        print('Epoch {}/{} ({:.2f} seconds)'.format(e, epochs, (time.time() - start)), end='\\n')\n",
    "    \n",
    "     \n",
    "    print('Training process for the ' + name + ' model was sucessful!')\n",
    "    \n",
    "    # Evaluate the model on test set\n",
    "    print('Start evaluating the ' + name + ' model on test set...')\n",
    "    test_datagen = ImageDataGenerator(rescale = 1./255)\n",
    "\n",
    "    test_generator = test_datagen.flow_from_directory(directory = 'augmented_dataset/test',                                                      batch_size = 1, color_mode=\"rgb\",                                                      class_mode = None,                                                      shuffle = False,                                                      seed = 42)\n",
    "\n",
    "    STEP_SIZE_TEST = test_generator.n // test_generator.batch_size\n",
    "\n",
    "    test_generator.reset()\n",
    "    y_pred = []\n",
    "    image_num = 0\n",
    "\n",
    "    for x_batch in test_generator:\n",
    "        image_num += 1\n",
    "        batch_x = [x.reshape(flat_dim) for x in x_batch]\n",
    "        y_pred.append(clf.predict(batch_x))\n",
    "\n",
    "        if image_num % 1000 == 0:\n",
    "            print(image_num, 'images processed')\n",
    "\n",
    "        if image_num >= test_generator.n:\n",
    "            break\n",
    "\n",
    "    print(image_num, 'images processed')    \n",
    "    print(name +' model was evaluated on test set successfully!')\n",
    "        \n",
    "    labels = (train_generator.class_indices)\n",
    "    labels = dict((v,k) for k, v in labels.items())\n",
    "    \n",
    "    return {'predictions' : [labels[k] for k in [x[0] for x in y_pred]], 'history': history}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# return the version of tensorflow\n",
    "def tf_version():\n",
    "    import tensorflow as tf\n",
    "    return tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
